{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Linear and Ploynomial Multivariate Regression\n",
    "\n",
    "This notebook estimates car MPG based on other data about the car. It receives its data from a CSV file (`auto-mpg.data`) and stores it in a Pandas DataFrame. Basic imputation is performed to remove the NaN values found in the horsepower column, and the data is standardized. Both a linear and polynomial multivariate regression algorithms are used to predict the MPG of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the file into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_frame(fname):\n",
    "    data = pd.read_table(fname, header=None, delim_whitespace=True,\n",
    "                         names=[\"mpg\", \"cylinders\", \"displacement\", \"horsepower\",\n",
    "                                \"weight\", \"acceleration\", \"model year\", \"origin\", \"car name\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_data_frame(\"auto-mpg.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of `auto-mpg.data`\n",
    "\n",
    "Contents are listed as pairs of column names and the type of data in the column:\n",
    "\n",
    "1. **mpg**:       continuous\n",
    "2. __cylinders__:    multi-valued discrete\n",
    "3. __displacement__:  continuous\n",
    "4. __horsepower__:    continuous\n",
    "5. __weight__:        continuous\n",
    "6. __acceleration__:  continuous\n",
    "7. __model year__:    multi-valued discrete\n",
    "8. __origin__:        multi-valued discrete\n",
    "9. __car name__:      string (unique for each instance)\n",
    "\n",
    "There are 398 rows (instances), each with these 9 attributes. The horsepower column is also known to have 6 NaN values.\n",
    "\n",
    "The following cell shows the first 10 rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "To impute the NaN values in the _horsepower_ column, replacement with the average value is used so that the data is not removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Nan(data):\n",
    "    num_cols = data.shape[1]\n",
    "    num_rows = data.shape[0]\n",
    "    for col in range(num_cols-1):\n",
    "        elem_list = []\n",
    "        col_sum = 0\n",
    "        num_items = 0\n",
    "        for row in range(num_rows):\n",
    "            if type(data.iloc[row, col]) is int or type(data.iloc[row, col]) is float:\n",
    "                if np.isnan(data.iloc[row, col]):\n",
    "                    elem_list.append((row, col))\n",
    "                else:\n",
    "                    col_sum += data.iloc[row, col]\n",
    "                    num_items += 1\n",
    "            elif type(data.iloc[row, col]) is str:\n",
    "                try:\n",
    "                    fdata = float(data.iloc[row, col])\n",
    "                except ValueError:\n",
    "                    fdata = np.nan\n",
    "                if np.isnan(fdata):\n",
    "                    elem_list.append((row, col))\n",
    "                else:\n",
    "                    data.iloc[row, col] = fdata\n",
    "                    col_sum += data.iloc[row, col]\n",
    "                    num_items += 1\n",
    "        if num_items > 0:\n",
    "            avg = col_sum / num_items\n",
    "            for r, c in elem_list:\n",
    "                data.iloc[r, c] = avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `car name` column is dropped as it provides no useful information for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_Nan(data)\n",
    "data = data.iloc[:, :-1]\n",
    "data[\"const\"] = 1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Statistics\n",
    "\n",
    "Statistics are obtained for the imputated data to aid in determining a standardization process. The following statistics are calculated:\n",
    "* Mean\n",
    "* Standard Deviation\n",
    "* Min/Max\n",
    "* Quartiles\n",
    "* Number of Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(data):\n",
    "    # Makes a 8x8 array of statistics\n",
    "    # Note: car names are excluded from this\n",
    "    stats = np.empty([8,8])\n",
    "    df = data.values[:,:-1]\n",
    "    inds = np.asarray(np.where(df == '?'))\n",
    "    for r, c in inds.T:\n",
    "        df[r, c] = np.nan\n",
    "    df = df.astype(float)\n",
    "    stats[:,0] = np.mean(df, axis=0)\n",
    "    stats[:,1] = np.std(df, axis=0)\n",
    "    stats[:,2] = df.min(axis=0)\n",
    "    stats[:,3] = df.max(axis=0)\n",
    "    stats[:,4] = np.percentile(df, 25, axis=0)\n",
    "    stats[:,5] = np.percentile(df, 50, axis=0)\n",
    "    stats[:,6] = np.percentile(df, 75, axis=0)\n",
    "    stats[:,7].fill(df.shape[0])\n",
    "    stats = pd.DataFrame(stats, index=data.columns[:-1], columns=[\"Mean\", \"Std\", \"Min\", \"Max\", \"25 Percentile\", \"50 Percentile\", \"75 Percentile\", \"Num Elems\"])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "For standardization, each value will be replaced with its z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data, stats):\n",
    "    for label in stats.index:\n",
    "        data[label] = data[label].apply(lambda x: (x - stats.loc[label, \"Mean\"]) / stats.loc[label, \"Std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "standardize(data, stats)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Sets\n",
    "\n",
    "The data is divided so that 80% of it is used for training, and the remaining 20% is used for testing.\n",
    "\n",
    "The data is divided randomly to prevent bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = data.shape[0]\n",
    "div = num_rows // 5\n",
    "train_max = 4 * div\n",
    "inds = np.random.choice(range(num_rows), size=train_max, replace=False)\n",
    "test_inds = [i for i in range(num_rows) if i not in inds]\n",
    "train = data.iloc[inds.tolist(), :]\n",
    "test = data.iloc[test_inds, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Inputs and Outputs\n",
    "\n",
    "The output data is separated from the input data, and all data is converted to `numpy` arrays of floats to simplify later calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[:, \"cylinders\":\"const\"].values.astype(float)\n",
    "r_train = train.loc[:, \"mpg\"].values.astype(float)\n",
    "X_test = test.loc[:, \"cylinders\":\"const\"].values.astype(float)\n",
    "r_test = test.loc[:, \"mpg\"].values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for Linear Regression\n",
    "\n",
    "A standard multivariate linear regression algorithm is used. The equation for the weights is as follows:\n",
    "$$\n",
    "w = (X^{T}X)^{-1}X^{T}r\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_train(X, r):\n",
    "    return np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures the input contains a 1 at its end to simplify the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_predict(X, weights):\n",
    "    if len(X) == len(weights):\n",
    "        X_pred = X[:]\n",
    "    elif len(X) == len(weights)-1:\n",
    "        X_pred = np.append(X, 1)\n",
    "    else:\n",
    "        raise TypeError(\"weights (size {}) and X (size {}) have incompatible sizes.\\nSizes should either be the same, or X should be one element smaller than weights.\".format(len(weights), len(X)))\n",
    "    return np.dot(weights, X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the mean squared error given input `X` and expected output `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_linreg(X, r, weights):\n",
    "    scores = []\n",
    "    for data, result in zip(X, r):\n",
    "        y = linreg_predict(data, weights)\n",
    "        scores.append((y-result)**2)\n",
    "    scores = np.array(scores)\n",
    "    lsquare_error = np.average(scores)\n",
    "    return lsquare_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = linreg_train(X_train, r_train)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsquare = error_linreg(X_train, r_train, weights)\n",
    "print(\"Mean Squared Error on Training = {}\".format(lsquare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Linear Regression\n",
    "\n",
    "Mean Squared Error will be used as the main testing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsquare_test = error_linreg(X_test, r_test, weights)\n",
    "print(\"Mean Squared Error on Testing = {}\".format(lsquare_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for Polynomial Regression\n",
    "\n",
    "The multivariate polynomial regression algorithm is implemented by calculating all powers of each variable from 1 up to the degree of the polynomial (i.e. for a quadratic regression, it calculates square of each feature and preserves the original values). It adds the extra data into a new data array. Then, the linear regression algorithm from above is applied to the expanded dataset to get the polynomial regression's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads in the same type of data that was passed to the linear regression algorithm and expands it to work for the polynomial regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_data_to_degree(data, degree=2):\n",
    "    try:\n",
    "        num_cols = data.shape[1]-1\n",
    "        final_data = np.empty((data.shape[0],0))\n",
    "        for col in range(num_cols):\n",
    "            for i in range(degree-1):\n",
    "                new_col = np.power(data[:, col], degree-i)\n",
    "                final_data = np.column_stack((final_data, new_col))\n",
    "            final_data = np.column_stack((final_data, data[:, col]))\n",
    "        final_data = np.column_stack((final_data, data[:, -1]))\n",
    "    except IndexError:\n",
    "        num_cols = len(data)-1\n",
    "        final_data = np.empty((0,))\n",
    "        for col in range(num_cols):\n",
    "            for i in range(degree-1):\n",
    "                new_col = data[col]**(degree-i)\n",
    "                final_data = np.append(final_data, new_col)\n",
    "            final_data = np.append(final_data, data[col])\n",
    "        final_data = np.append(final_data, data[-1])\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyreg_train(X, r, degree=2):\n",
    "    X_poly = _expand_data_to_degree(X, degree)\n",
    "    return (linreg_train(X_poly, r), degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyreg_predict(X, weights):\n",
    "    X_poly = _expand_data_to_degree(X, weights[1])\n",
    "    return linreg_predict(X_poly, weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the mean squared error for the provided model given input `X` and expected output `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_polyreg(X, r, weights):\n",
    "    scores = []\n",
    "    for data, result in zip(X, r):\n",
    "        y = polyreg_predict(data, weights)\n",
    "        scores.append((y-result)**2)\n",
    "    scores = np.array(scores)\n",
    "    lsquare_error = np.average(scores)\n",
    "    return lsquare_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X_train, r_train):\n",
    "    train_errors = []\n",
    "    valid_errors = []\n",
    "    for degree in range(1, 5):\n",
    "        train_errors.append([])\n",
    "        valid_errors.append([])\n",
    "    for i in range(10):\n",
    "        num_tr = X_train.shape[0]\n",
    "        div_tr = num_rows // 4\n",
    "        tr_max = 3 * div\n",
    "        tr_inds = np.random.choice(range(num_tr), size=tr_max, replace=False)\n",
    "        tr_valid_inds = [i for i in range(num_tr) if i not in tr_inds]\n",
    "        X_train_tr = X_train[tr_inds, :]\n",
    "        X_valid_tr = X_train[tr_valid_inds, :]\n",
    "        r_train_tr = r_train[tr_inds]\n",
    "        r_valid_tr = r_train[tr_valid_inds]\n",
    "        for degree in range(1, 5):\n",
    "            poly_weights = polyreg_train(X_train_tr, r_train_tr, degree=degree)\n",
    "            train_error = error_polyreg(X_train_tr, r_train_tr, poly_weights)\n",
    "            train_errors[degree-1].append(train_error)\n",
    "            valid_error = error_polyreg(X_valid_tr, r_valid_tr, poly_weights)\n",
    "            valid_errors[degree-1].append(valid_error)\n",
    "    avg_train_errors = []\n",
    "    avg_valid_errors = []\n",
    "    for te, ve in zip(train_errors, valid_errors):\n",
    "        te = np.array(te)\n",
    "        ve = np.array(ve)\n",
    "        avg_train_errors.append(np.average(te))\n",
    "        avg_valid_errors.append(np.average(ve))\n",
    "    avg_train_errors = np.array(avg_train_errors)\n",
    "    avg_valid_errors = np.array(avg_valid_errors)\n",
    "    print(\"Average Training Errors: {}\".format(avg_train_errors))\n",
    "    print(\"Average Validation Errors: {}\".format(avg_valid_errors))\n",
    "    comp_matrix = np.empty((4,4))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            comp = avg_valid_errors[i] - avg_valid_errors[j]\n",
    "            if comp < 0.005:\n",
    "                comp_matrix[i, j] = 0\n",
    "            else:\n",
    "                comp_matrix[i, j] = comp\n",
    "    best_degree = 0\n",
    "    all_neg = False\n",
    "    second_zero = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if comp_matrix[i, j] < 0:\n",
    "                all_neg = True\n",
    "            elif comp_matrix[i, j] == 0:\n",
    "                second_zero = j+1\n",
    "                all_neg = True\n",
    "            else:\n",
    "                all_neg = False\n",
    "                break\n",
    "        if all_neg:\n",
    "            if second_zero > 0 and second_zero < i+1:\n",
    "                best_degree = j+1\n",
    "            else:\n",
    "                best_degree = i+1\n",
    "            break\n",
    "    print(\"Cross Validation suggests the best polynomial degree is {}\".format(best_degree))\n",
    "    return best_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = cross_validate(X_train, r_train)\n",
    "poly_weights = polyreg_train(X_train, r_train, degree=degree)\n",
    "print(\"\\nPolynomial Weights: {}\".format(poly_weights))\n",
    "lsquare_poly = error_polyreg(X_train, r_train, poly_weights)\n",
    "print(\"Mean Squared Error on Training = {}\".format(lsquare_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Polynomial Regression\n",
    "Mean Squared Error will be used as the main testing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsquare_testpoly = error_polyreg(X_test, r_test, poly_weights)\n",
    "print(\"Mean Squared Error on Testing = {}\".format(lsquare_testpoly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Feature Importance\n",
    "\n",
    "To see how important a feature is, columns are progressively removed from the dataset, and errors are obtained and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_analysis(X, r, degree):\n",
    "    train_errors = []\n",
    "    valid_errors = []\n",
    "    for i in range(X.shape[1]):\n",
    "        X_train = X[i:]\n",
    "        num_tr = X_train.shape[0]\n",
    "        div_tr = num_rows // 4\n",
    "        tr_max = 3 * div\n",
    "        tr_inds = np.random.choice(range(num_tr), size=tr_max, replace=False)\n",
    "        tr_valid_inds = [i for i in range(num_tr) if i not in tr_inds]\n",
    "        X_train_tr = X_train[tr_inds, :]\n",
    "        X_valid_tr = X_train[tr_valid_inds, :]\n",
    "        r_train_tr = r[tr_inds]\n",
    "        r_valid_tr = r[tr_valid_inds]\n",
    "        weights = polyreg_train(X_train_tr, r_train_tr, degree=degree)\n",
    "        te = error_polyreg(X_train_tr, r_train_tr, weights)\n",
    "        ve = error_polyreg(X_valid_tr, r_valid_tr, weights)\n",
    "        train_errors.append(te)\n",
    "        valid_errors.append(ve)\n",
    "    print(\"Training Errors by Starting Column:\\n  {}\\n\".format(train_errors))\n",
    "    print(\"Validation Errors by Starting Column:\\n  {}\\n\".format(valid_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_analysis(X_train, r_train, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
